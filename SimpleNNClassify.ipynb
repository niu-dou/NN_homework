{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载CIFAR-10数据集\n",
    "def load_cifar10_batch(file):\n",
    "    \"\"\"加载单个CIFAR-10数据批次\"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        dict = pickle.load(f, encoding='bytes')\n",
    "    return dict[b'data'], dict[b'labels']\n",
    "\n",
    "def load_cifar10_data(data_dir):\n",
    "    \"\"\"加载CIFAR-10训练和测试数据\"\"\"\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for i in range(1, 6):\n",
    "        data, labels = load_cifar10_batch(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "        train_data.append(data)\n",
    "        train_labels.extend(labels)\n",
    "    train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1).reshape(-1, 3072)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    test_data, test_labels = load_cifar10_batch(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1).reshape(-1, 3072)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(train_data, test_data):\n",
    "    \"\"\"将数据归一化到[0, 1]范围\"\"\"\n",
    "    train_data = train_data / 255.0\n",
    "    test_data = test_data / 255.0\n",
    "    return train_data, test_data\n",
    "\n",
    "# 激活函数\n",
    "def relu(x):\n",
    "    \"\"\"ReLU激活函数\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLU激活函数的导数\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax函数\"\"\"\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# 损失函数\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"交叉熵损失函数\"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true] + 1e-9)  # 加小值防止log(0)\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "# 准确率计算\n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"计算分类准确率\"\"\"\n",
    "    predictions = np.argmax(y_pred, axis=1)\n",
    "    return np.mean(predictions == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型类\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, activation='relu'):\n",
    "        \"\"\"初始化三层神经网络\"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1) if self.activation == 'relu' else np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = relu(self.z2) if self.activation == 'relu' else np.tanh(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = softmax(self.z3)\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, X, y_one_hot, output, learning_rate, reg_lambda):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        dz3 = output - y_one_hot\n",
    "        dW3 = np.dot(self.a2.T, dz3) / m + reg_lambda * self.W3\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        da2 = np.dot(dz3, self.W3.T)\n",
    "        dz2 = da2 * relu_derivative(self.z2) if self.activation == 'relu' else da2 * (1 - self.a2**2)\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m + reg_lambda * self.W2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * relu_derivative(self.z1) if self.activation == 'relu' else da1 * (1 - self.a1**2)\n",
    "        dW1 = np.dot(X.T, dz1) / m + reg_lambda * self.W1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # 更新权重\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 辅助函数：将标签转换为one-hot编码\n",
    "def to_one_hot(labels, num_classes):\n",
    "    \"\"\"将标签转换为one-hot编码\"\"\"\n",
    "    m = len(labels)\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    one_hot[np.arange(m), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "# 保存模型参数\n",
    "def save_model(model, filename='best_model.pkl'):\n",
    "    \"\"\"保存模型的参数到文件\"\"\"\n",
    "    model_params = {\n",
    "        'W1': model.W1,\n",
    "        'b1': model.b1,\n",
    "        'W2': model.W2,\n",
    "        'b2': model.b2,\n",
    "        'W3': model.W3,\n",
    "        'b3': model.b3\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    print(f'Model saved to {filename}')\n",
    "\n",
    "# 加载模型参数\n",
    "def load_model(model, filename='best_model.pkl'):\n",
    "    \"\"\"从文件加载模型的参数\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        model_params = pickle.load(f)\n",
    "    model.W1 = model_params['W1']\n",
    "    model.b1 = model_params['b1']\n",
    "    model.W2 = model_params['W2']\n",
    "    model.b2 = model_params['b2']\n",
    "    model.W3 = model_params['W3']\n",
    "    model.b3 = model_params['b3']\n",
    "    print(f'Model loaded from {filename}')\n",
    "\n",
    "# 新增：绘制损失和准确率曲线的函数\n",
    "def plot_results(train_losses, val_losses, val_accs):\n",
    "    \"\"\"绘制训练过程中的损失和验证准确率曲线\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(12, 5))  # 设置图形大小\n",
    "    plt.subplot(1, 2, 1)  # 两张图并排，第一张\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')  # X轴标签\n",
    "    plt.ylabel('Loss')   # Y轴标签\n",
    "    plt.title('Loss Curve')  # 图标题\n",
    "    plt.legend()  # 显示图例\n",
    "    \n",
    "    # 绘制验证准确率曲线\n",
    "    plt.subplot(1, 2, 2)  # 两张图并排，第二张\n",
    "    plt.plot(epochs, val_accs, label='Validation Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')  # X轴标签\n",
    "    plt.ylabel('Accuracy')  # Y轴标签\n",
    "    plt.title('Validation Accuracy Curve')  # 图标题\n",
    "    plt.legend()  # 显示图例\n",
    "    \n",
    "    plt.tight_layout()  # 调整布局以避免重叠\n",
    "    plt.show()  # 显示图形\n",
    "\n",
    "def visualize_parameters(model):\n",
    "    \"\"\"可视化神经网络的参数（权重和偏置）\"\"\"\n",
    "    # 提取参数\n",
    "    W1, b1 = model.W1, model.b1\n",
    "    W2, b2 = model.W2, model.b2\n",
    "    W3, b3 = model.W3, model.b3\n",
    "    \n",
    "    # 创建一个包含3行2列的子图布局\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "    fig.suptitle('Neural Network Parameters Visualization')\n",
    "    \n",
    "    # 绘制W1的直方图\n",
    "    axes[0, 0].hist(W1.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "    axes[0, 0].set_title('W1 Weights Histogram')\n",
    "    axes[0, 0].set_xlabel('Weight Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 绘制b1的直方图\n",
    "    axes[0, 1].hist(b1.flatten(), bins=50, color='green', alpha=0.7)\n",
    "    axes[0, 1].set_title('b1 Biases Histogram')\n",
    "    axes[0, 1].set_xlabel('Bias Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 绘制W2的直方图\n",
    "    axes[1, 0].hist(W2.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "    axes[1, 0].set_title('W2 Weights Histogram')\n",
    "    axes[1, 0].set_xlabel('Weight Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 绘制b2的直方图\n",
    "    axes[1, 1].hist(b2.flatten(), bins=50, color='green', alpha=0.7)\n",
    "    axes[1, 1].set_title('b2 Biases Histogram')\n",
    "    axes[1, 1].set_xlabel('Bias Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 绘制W3的直方图\n",
    "    axes[2, 0].hist(W3.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "    axes[2, 0].set_title('W3 Weights Histogram')\n",
    "    axes[2, 0].set_xlabel('Weight Value')\n",
    "    axes[2, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 绘制b3的直方图\n",
    "    axes[2, 1].hist(b3.flatten(), bins=50, color='green', alpha=0.7)\n",
    "    axes[2, 1].set_title('b3 Biases Histogram')\n",
    "    axes[2, 1].set_xlabel('Bias Value')\n",
    "    axes[2, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 调整布局\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "def visualize_weight_heatmap(model, layer=1):\n",
    "    \"\"\"可视化指定层的权重热力图\"\"\"\n",
    "    if layer == 1:\n",
    "        W = model.W1\n",
    "        title = 'W1 Weights Heatmap'\n",
    "    elif layer == 2:\n",
    "        W = model.W2\n",
    "        title = 'W2 Weights Heatmap'\n",
    "    elif layer == 3:\n",
    "        W = model.W3\n",
    "        title = 'W3 Weights Heatmap'\n",
    "    else:\n",
    "        raise ValueError(\"Layer must be 1, 2, or 3\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(W, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Neurons in Next Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练函数\n",
    "def train(model, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate, reg_lambda, lr_decay=1.0, search=False):\n",
    "    \"\"\"训练模型，并在验证集上表现最佳时保存模型，并返回损失和准确率数据\"\"\"\n",
    "    m = X_train.shape[0]\n",
    "    y_train_one_hot = to_one_hot(y_train, 10)\n",
    "    y_val_one_hot = to_one_hot(y_val, 10)\n",
    "    \n",
    "    best_val_acc = 0  # 记录最佳验证准确率\n",
    "    train_losses = []  \n",
    "    val_losses = []    \n",
    "    val_accs = []     \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 打乱数据\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_train_shuffled = X_train[permutation]\n",
    "        y_train_shuffled = y_train_one_hot[permutation]\n",
    "        \n",
    "        # 批量训练\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_train_shuffled[i:i+batch_size]\n",
    "            y_batch = y_train_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model.forward(X_batch)\n",
    "            \n",
    "            # 反向传播\n",
    "            model.backward(X_batch, y_batch, output, learning_rate, reg_lambda)\n",
    "        \n",
    "        # 学习率衰减\n",
    "        learning_rate *= lr_decay\n",
    "        \n",
    "        # 计算损失和准确率\n",
    "        train_output = model.forward(X_train)\n",
    "        train_loss = cross_entropy_loss(train_output, y_train)\n",
    "        train_acc = accuracy(train_output, y_train)\n",
    "        \n",
    "        val_output = model.forward(X_val)\n",
    "        val_loss = cross_entropy_loss(val_output, y_val)\n",
    "        val_acc = accuracy(val_output, y_val)\n",
    "        \n",
    "        # 记录数据\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        \n",
    "        if not search: # 超参数搜索时不做检查\n",
    "            if val_acc > best_val_acc: # 检查是否为最佳模型\n",
    "                best_val_acc = val_acc\n",
    "                save_model(model, 'best_model.pkl')\n",
    "                print(f'New best model saved with Val Acc: {best_val_acc:.4f}')\n",
    "    \n",
    "    # 返回记录的数据以供绘图\n",
    "    return train_losses, val_losses, val_accs\n",
    "\n",
    "# 测试函数\n",
    "def test(model, X_test, y_test):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    output = model.forward(X_test)\n",
    "    acc = accuracy(output, y_test)\n",
    "    print(f'Test Accuracy: {acc:.4f}')\n",
    "\n",
    "# 超参数搜索\n",
    "def hyperparameter_search(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"超参数网格搜索\"\"\"\n",
    "    best_acc = 0\n",
    "    best_params = {}\n",
    "    for hidden1_size,hidden2_size in [(256,128),(512,256)]:\n",
    "        for reg_lambda in [0.001, 0.01]:\n",
    "            for l_r in [0.02,0.01]:\n",
    "                for lr_decay in [0.99,0.98,0.96]:\n",
    "                    print(f'Trying hidden1_size={hidden1_size}, hidden2_size={hidden2_size}, reg_lambda={reg_lambda}, learning_rate={l_r}, lr_decay={lr_decay}')\n",
    "                    model = NeuralNetwork(3072, hidden1_size, hidden2_size, 10)\n",
    "                    train_losses, val_losses, val_accs = train(model, X_train, y_train, X_val, y_val, epochs=30, batch_size=64, learning_rate=0.01, reg_lambda=reg_lambda, search=True)\n",
    "                    val_output = model.forward(X_val)\n",
    "                    val_acc = accuracy(val_output, y_val)\n",
    "                    if val_acc > best_acc:\n",
    "                        best_acc = val_acc\n",
    "                        best_params = {'hidden1_size': hidden1_size, 'hidden2_size': hidden2_size, 'reg_lambda': reg_lambda, \"learning_rate\": l_r, \"lr_decay\": lr_decay}\n",
    "                    # 在超参数搜索中也绘制曲线（可选）\n",
    "                    plot_results(train_losses, val_losses, val_accs)\n",
    "    print(f'Best Validation Accuracy: {best_acc:.4f} with params: {best_params}')\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 请替换为你的CIFAR-10数据集路径\n",
    "    data_dir = 'path/to/cifar-10-batches-py'\n",
    "    train_data, train_labels, test_data, test_labels = load_cifar10_data(data_dir)\n",
    "    train_data, test_data = preprocess_data(train_data, test_data)\n",
    "    \n",
    "    # 分割验证集\n",
    "    val_size = 5000\n",
    "    X_val = train_data[:val_size]\n",
    "    y_val = train_labels[:val_size]\n",
    "    X_train = train_data[val_size:]\n",
    "    y_train = train_labels[val_size:]\n",
    "    \n",
    "    # 超参数搜索\n",
    "    best_params = hyperparameter_search(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # 使用最佳超参数训练模型\n",
    "    model = NeuralNetwork(3072, best_params['hidden1_size'], best_params['hidden2_size'], 10)\n",
    "    train_losses, val_losses, val_accs = train(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, learning_rate=best_params['learning_rate'], \n",
    "                                               reg_lambda=best_params['reg_lambda'], lr_decay=best_params['lr_decay'])\n",
    "    \n",
    "    # 绘制训练结果\n",
    "    plot_results(train_losses, val_losses, val_accs)\n",
    "    \n",
    "    # 可视化参数\n",
    "    visualize_parameters(model)  # 绘制直方图\n",
    "    visualize_weight_heatmap(model, layer=1)  # 绘制第一层权重热力图\n",
    "    visualize_weight_heatmap(model, layer=2)  # 绘制第二层权重热力图\n",
    "    visualize_weight_heatmap(model, layer=3)  # 绘制第三层权重热力图\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    load_model(model, 'best_model.pkl')\n",
    "    \n",
    "    # 测试模型\n",
    "    test(model, test_data, test_labels)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
